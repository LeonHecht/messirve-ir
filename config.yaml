experiment:
  checkpoint: "sentence-transformers/distiluse-base-multilingual-cased-v2"  # Pretrained model checkpoint
  dataset_name: "messirve_ar_hard_negatives5_sbert"  # Dataset name (used for logging, not loading)
  loss_name: "MultipleNegativesRankingLoss"  # Loss function name (used for logging)
  learning_rate: 2e-5  # Initial learning rate
  batch_size: 16  # Training and evaluation batch size
  num_epochs: 2  # Total number of training epochs
  warmup_ratio: 0.1  # Warmup ratio for the learning rate scheduler
  weight_decay: 0.01  # Weight decay (null = no weight decay)
  max_grad_norm: 1000  # Maximum gradient norm for clipping (null = no clipping)
  fp16: False  # Enable mixed precision (FP16) training
  bf16: True  # Enable BF16 precision training (overrides FP16 if supported)
  eval_steps: 100  # Perform evaluation every N steps
  save_steps: 100  # Save the model every N steps